{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustring\n",
    "Nous voulons savoir les sujets les plus abordés dans les news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Décompréssez le doc bbcsport.zip sur votre machine et utilisez le bon path pour le dossier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val corpus = sc.wholeTextFiles(\"/path/bbcsport/*/*.txt\").map(_._2).map(_.toLowerCase())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparer le RDD et ajouter un ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Split document by double newlines, drop the first block, combine again as a string\n",
    "val corpus_body = corpus.map(_.split(\"\\\\n\\\\n\")).map(_.drop(1)).map(_.mkString(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Convert RDD to DF with ID for every document\n",
    "val corpus_df = corpus_body.zipWithIndex.toDF(\"corpus\", \"id\")\n",
    "corpus_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# les Transformateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "// Set params for RegexTokenizer\n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setPattern(\"[\\\\W_]+\")\n",
    "  .setMinTokenLength(4) // Filter away tokens with length < 4\n",
    "  .setInputCol(\"corpus\")\n",
    "  .setOutputCol(\"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWordsRemover supprime les mots non pertinants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez le bon path pour le fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val stopwords = sc.textFile(\"/path/stop_words\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "// Set params for StopWordsRemover\n",
    "val remover = new StopWordsRemover()\n",
    "  .setStopWords(stopwords) // This parameter is optional\n",
    "  .setInputCol(\"tokens\")\n",
    "  .setOutputCol(\"filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "// Set params for CountVectorizer\n",
    "val vectorizer = new CountVectorizer()\n",
    "  .setInputCol(\"filtered\")\n",
    "  .setOutputCol(\"features\")\n",
    "  .setVocabSize(10000)\n",
    "  .setMinDF(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Clustring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val numTopics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.clustering.LDA\n",
    "\n",
    "val lda = new LDA()\n",
    "  .setK(numTopics)\n",
    "  .setMaxIter(50)\n",
    "  .setOptimizer(\"em\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer et ajuster  Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, remover, vectorizer, lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.write.overwrite().save(\"/tmp/ldaDemo/pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pipelineModel = pipeline.fit(corpus_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons extraire des étapes particulières si nécessaire.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val vectorizerModel = pipelineModel.stages(2).asInstanceOf[CountVectorizerModel]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.clustering.DistributedLDAModel\n",
    "\n",
    "// Since we trained with the default optimizer (EM), we get back a DistributedLDAModel\n",
    "val ldaModel = pipelineModel.stages(3).asInstanceOf[DistributedLDAModel]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Sauvegardons notre modèle LDA pour une réutilisation ultérieure.\n",
    "ldaModel.write.overwrite().save(\"/tmp/ldaDemo/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Data log likelihood gives us a statistic for evaluation.\n",
    "// This statistics is always negative, and closer to 0 is better.\n",
    "ldaModel.trainingLogLikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichez les sujets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Get vocab\n",
    "val vocabList = vectorizerModel.vocabulary\n",
    "val termsIdx2Str = udf { (termIndices: Seq[Int]) => termIndices.map(idx => vocabList(idx)) }\n",
    "\n",
    "// Review Results of LDA model with Online Variational Bayes\n",
    "val topics = ldaModel.describeTopics(maxTermsPerTopic = 5)\n",
    "  .withColumn(\"terms\", termsIdx2Str(col(\"termIndices\")))\n",
    "topics.select(\"topic\", \"terms\", \"termWeights\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create DF with proper column names\n",
    "//val termDF = termRDD2.toDF.withColumnRenamed(\"_1\", \"term\").withColumnRenamed(\"_2\", \"probability\").withColumnRenamed(\"_3\", \"topicId\")\n",
    "val zipUDF = udf { (terms: Seq[String], probabilities: Seq[Double]) => terms.zip(probabilities) }\n",
    "val topicsTmp = topics.withColumn(\"termWithProb\", explode(zipUDF(col(\"terms\"), col(\"termWeights\"))))\n",
    "val termDF = topicsTmp.select(\n",
    "  col(\"topic\").as(\"topicId\"),\n",
    "  col(\"termWithProb._1\").as(\"term\"),\n",
    "  col(\"termWithProb._2\").as(\"probability\"))\n",
    "termDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
