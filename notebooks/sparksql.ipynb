{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Le point d’entrée pour la programmation de Spark avec l’API Dataset et DataFrame.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## point d’entrée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <p>Un point d'entrée est l'endroit où le contrôle est transféré du système d'exploitation au programme fourni.</p><br>\n",
    "<p>Pour créer une SparkSession de base, utilisez SparkSession.builder():</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark SQL basic example\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Avec une SparkSession, les applications peuvent créer des DataFrames à partir d'un RDD existant, d'une table Hive ou de sources de données Spark.\n",
    "<br>\n",
    "À titre d'exemple, ce qui suit crée un DataFrame basé sur le contenu d'un fichier CSV:</p>\n",
    "<h3 style=\"color:red;\">n'oubliez pas de mettre le bon path to spark folder </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.json(\"/path to spark folder/examples/src/main/resources/people.json\")\n",
    "\n",
    "// Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opérations DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Les DataFrames fournissent un langage spécifique  pour la manipulation de données structurées en Scala, Java, Python et R.\n",
    "Ces opérations sont également appelées «transformations non typées», contrairement aux «transformations typées» fournies avec des Dataset Scala / Java fortement typés.\n",
    "Nous incluons ici quelques exemples de base de traitement de données structurées utilisant des ensembles de données:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>printSchema imprime le schéma d'un DataFrame dans un format d'arborescence.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction select sélectionne les colonnes spécifiées et les renvoie sous la forme d'un nouveau DataFrame. Ceci est similaire à l'instruction SELECT en SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélectionnez uniquement la colonne \"nom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélectionnez tout le monde, mais incrémentez l'âge de 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df(\"name\"), df(\"age\") + 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le filtre retourne les lignes d'un DataFrame correspondant à la condition donnée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélectionnez les personnes de plus de 21 ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter($\"age\" > 21).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction groupBy regroupe votre DataFrame sur la ou les colonnes spécifiées afin que vous puissiez y exécuter des agrégations, un peu comme votre instruction SQL GROUP BY. La fonction groupBy génère un objet GroupedData qui peut ensuite être transmis aux fonctions d'agrégation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compter les gens par âge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exécution de requêtes SQL par programme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction sql sur SparkSession permet aux applications d'exécuter des requêtes SQL par programme et renvoie le résultat sous la forme d'un DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## createOrReplaceTempView"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crée une nouvelle vue temporaire à l'aide de SparkDataFrame dans la session Spark. Si une vue temporaire portant le même nom existe déjà, la remplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"people\")\n",
    "val sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Temporary View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les vues temporaires dans Spark SQL ont une portée de session et disparaissent si la session qui les crée se termine. Si vous souhaitez avoir une vue temporaire partagée entre toutes les sessions et rester en vie jusqu'à la fin de l'application Spark, vous pouvez créer une vue temporaire globale. La vue temporaire globale est liée à une base de données préservée par le système, global_temp, et nous devons utiliser le nom qualifié pour la référencer, par exemple. SELECT * FROM global_temp.view1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Enregistrer le DataFrame en tant que vue temporaire globale\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "// La vue temporaire globale est liée à une base de données préservée du système, `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "\n",
    "\n",
    "// La vue temporaire globale est intersession\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les ensembles de données sont similaires aux RDD. Cependant, au lieu d'utiliser la sérialisation Java, ils utilisent un encodeur spécialisé pour sérialiser les objets à traiter ou à transmettre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une class case Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Person(name: String, age: Long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encodage  dune seq  d'objets de type Person  en Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val caseClassDS = Seq(Person(\"Andy\", 32)).toDS()\n",
    "caseClassDS.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les encodeurs pour les types les plus courants sont automatiquement fournis en important spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val primitiveDS = Seq(1, 2, 3).toDS()\n",
    "primitiveDS.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les DataFrames peuvent être convertis en un jeu de données en fournissant une classe. La cartographie sera faite par nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val peopleDS = df.as[Person]\n",
    "peopleDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
