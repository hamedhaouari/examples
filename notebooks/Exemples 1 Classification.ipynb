{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Nous allons utiliser Un BBC news sport Dataset l'objectif est de créer un model qui detecte les news parlant du foot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notre flux de  travail\n",
    "1- Lire Les documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Préparez-les à la classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Former un clas ificateur  (notre modèle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Tester le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de la session spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark SQL basic example\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lire les  documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Décompréssez le doc bbcsport.zip sur votre machine et utilisez le bon path pour le dossier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val newsRDD = sc.wholeTextFiles(\"/path/bbcsport/football/*.txt\")\n",
    "val newsRDD1 = sc.wholeTextFiles(\"/path/bbcsport-fulltext/bbcsport/tennis/*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du type news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class news( path : String , text : String, sport : String , label : Double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d un dataframe englobant les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val newdf1 = newsRDD.map(x => news(x._1 , x._2 , \"foot\", 1.0)).toDF()\n",
    "newdf1.show()\n",
    "val newdf2 = newsRDD1.map(x => news(x._1 , x._2 , \"tennis\", 0.0)).toDF()\n",
    "newdf2.show()\n",
    "import org.apache.spark.sql.functions.rand\n",
    "val newdf3 = newdf1.union(newdf2).orderBy(rand())\n",
    "newdf3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation du Fataframe en Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val newds= newdf3.as[news]\n",
    "newds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import des classe mllib et décomposition du dataset en données d'entrainement et données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "val Array(training, test) = newds.randomSplit(Array[Double](0.7, 0.3), 12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show()\n",
    "test.count()\n",
    "training.show()\n",
    "training.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notre  pipeline\n",
    "<img src=\"aa.png\" ></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformateurs\n",
    "<pre>\n",
    "\n",
    "Tockenizer\n",
    "Le processus de prise de texte (généralement sous forme de documents ou de  phrases) et de rupture de texte en termes (généralement des  mots)\n",
    "\n",
    "HashingTF est un transformateur qui prend des ensembles de termes et convertit ces ensembles en vecteurs de longueur fixe.\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val tokenizer = new Tokenizer()\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val hashingTF = new HashingTF()\n",
    "  .setNumFeatures(1000)\n",
    "  .setInputCol(tokenizer.getOutputCol)\n",
    "  .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Estimateur que nous sommes en train d'utiliser\n",
    "Régression logistique\n",
    "<pre>\n",
    "La Régression logistique est une méthode populaire pour prédire une réponse catégorique. elle est un cas particulier des modèles linéaires généralisés qui prédit la probabilité des résultats. Dans spark.ml, la régression logistique peut être utilisée pour prédire un  un résultat multiclasse en utilisant la régression logistique binomiale, ou elle peut être utilisé pour prédire un résultat multiclasse en utilisant la régression logistique  multinomiale. \n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lr = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.001)\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer, hashingTF, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer et ajuster  Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Fit the pipeline to training documents.\n",
    "val model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now we can optionally save the fitted pipeline to disk\n",
    "model.write.overwrite().save(\"/tmp/spark-logistic-regression-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// We can also save this unfit pipeline to disk\n",
    "pipeline.write.overwrite().save(\"/tmp/unfit-lr-model\")\n",
    "\n",
    "// And load it back in during production\n",
    "val sameModel = PipelineModel.load(\"/tmp/spark-logistic-regression-model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appliquer le model sur la data de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val predections = model.transform(test)\n",
    "predections.select(\"path\", \"text\", \"sport\",\"probability\", \"prediction\")\n",
    "  .collect()\n",
    "  .foreach { case Row(path: String, text: String,sport:String, prob: Vector, prediction: Double) =>\n",
    "    println(s\"($sport) -->  prediction=$prediction\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluer le model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "val eval = new BinaryClassificationEvaluator() \n",
    "eval.evaluate(predections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
