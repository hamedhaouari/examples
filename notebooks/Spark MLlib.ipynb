{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib Statistiques de base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrélation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le calcul de la corrélation entre les deux séries de données est une opération courante dans la statistique. SparkMLlib donne la possibilité de calculer des corrélations par paires parmi de nombreuses séries. Les méthodes de corrélation prises en charge sont actuellement la corrélation de <b>Pearson et Spearman.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La corrélation calcule la matrice de corrélation pour le DataSet de vecteurs en entrée à l'aide de la méthode spécifiée. La sortie sera un DataFrame contenant la matrice de corrélation de la colonne de vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
    "import org.apache.spark.ml.stat.Correlation\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val data = Seq(\n",
    "  Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))),\n",
    "  Vectors.dense(4.0, 5.0, 0.0, 3.0),\n",
    "  Vectors.dense(6.0, 7.0, 0.0, 8.0),\n",
    "  Vectors.sparse(4, Seq((0, 9.0), (3, 1.0)))\n",
    ")\n",
    "\n",
    "val df = data.map(Tuple1.apply).toDF(\"features\")\n",
    "val Row(coeff1: Matrix) = Correlation.corr(df, \"features\").head\n",
    "println(s\"Pearson correlation matrix:\\n $coeff1\")\n",
    "\n",
    "val Row(coeff2: Matrix) = Correlation.corr(df, \"features\", \"spearman\").head\n",
    "println(s\"Spearman correlation matrix:\\n $coeff2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests d'hypothèses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le test d'hypothèses est un outil puissant en statistiques pour déterminer si un résultat est statistiquement significatif, si ce résultat est dû au hasard ou non. spark.ml prend actuellement en charge le chi carré de Pearson des tests d'indépendance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.stat.ChiSquareTest\n",
    "\n",
    "val data = Seq(\n",
    "  (0.0, Vectors.dense(0.5, 10.0)),\n",
    "  (0.0, Vectors.dense(1.5, 20.0)),\n",
    "  (1.0, Vectors.dense(1.5, 30.0)),\n",
    "  (0.0, Vectors.dense(3.5, 30.0)),\n",
    "  (0.0, Vectors.dense(3.5, 40.0)),\n",
    "  (1.0, Vectors.dense(3.5, 40.0))\n",
    ")\n",
    "\n",
    "val df = data.toDF(\"label\", \"features\")\n",
    "val chi = ChiSquareTest.test(df, \"features\", \"label\").head\n",
    "println(s\"pValues = ${chi.getAs[Vector](0)}\")\n",
    "println(s\"degreesOfFreedom ${chi.getSeq[Int](1).mkString(\"[\", \",\", \"]\")}\")\n",
    "println(s\"statistics ${chi.getAs[Vector](2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# statistiques synthétiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkMLlib fournit des statistiques de synthèse de colonne de vecteur pour Dataframe via Summarizer. Les mesures disponibles sont les valeurs maximales, minimales, moyennes, de variance et le nombre de nonzeros dans les colonnes, ainsi que le nombre total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "//import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n",
    "import org.apache.spark.ml.stat.Summarizer\n",
    "\n",
    " val data = Seq(\n",
    "      (Vectors.dense(2.0, 3.0, 5.0), 1.0),\n",
    "      (Vectors.dense(4.0, 6.0, 7.0), 2.0)\n",
    "    )\n",
    "\n",
    "    val df = data.toDF(\"features\", \"weight\")\n",
    "\n",
    "    val (meanVal, varianceVal) = df.select(Summarizer.metrics(\"mean\", \"variance\")\n",
    "      .summary($\"features\", $\"weight\").as(\"summary\"))\n",
    "      .select(\"summary.mean\", \"summary.variance\")\n",
    "      .as[(Vector, Vector)].first()\n",
    "\n",
    "    println(s\"with weight: mean = ${meanVal}, variance = ${varianceVal}\")\n",
    "\n",
    "    val (meanVal2, varianceVal2) = df.select(Summarizer.mean($\"features\"), Summarizer.variance($\"features\"))\n",
    "      .as[(Vector, Vector)].first()\n",
    "\n",
    "    println(s\"without weight: mean = ${meanVal2}, sum = ${varianceVal2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
