{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pré requis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devez d’abord exécuter Netcat (un petit utilitaire présent dans la plupart des systèmes de type Unix) en tant que serveur de données à l’aide de\n",
    "\n",
    "<code>$ nc -lk 9999</code>\n",
    "\n",
    "<h2> après avoir executer toute les cellule copiez le texte que vous voulez dans la terminal netct, spark va lire les données et afficher le résultat </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premièrement, nous importons les noms des classes Spark Streaming et certaines conversions implicites de StreamingContext dans notre environnement afin d’ajouter des méthodes utiles aux autres classes dont nous avons besoin (comme DStream). StreamingContext est le point d'entrée principal pour toutes les fonctionnalités de diffusion en continu. Nous créons un StreamingContext local avec deux threads d'exécution et un intervalle de traitement par lots d'une seconde.<br>\n",
    "<b> c'est un streamingcontext qui cherche les nouvelle données toute les secondes </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3\n",
    "\n",
    "// Create a local StreamingContext with two working thread and batch interval of 1 second.\n",
    "// The master requires 2 cores to prevent a starvation scenario.\n",
    "\n",
    "val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkWordCount\")\n",
    "val ssc = new StreamingContext(conf, Seconds(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant ce contexte, nous pouvons créer un DStream qui représente les données en continu d'une source TCP, spécifiées comme nom d'hôte (par exemple localhost) et port (par exemple 9999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "val lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette ligne DStream représente le flux de données qui sera reçu du serveur de données. Chaque enregistrement de ce DStream est une ligne de texte. Ensuite, nous voulons diviser les lignes par des caractères d'espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Split each line into words\n",
    "val words = lines.flatMap(_.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatMap est une opération DStream d'un à plusieurs qui crée un nouveau DStream en générant plusieurs nouveaux enregistrements à partir de chaque enregistrement du DStream source. Dans ce cas, chaque ligne sera scindée en plusieurs mots et le flux de mots est représenté par les mots DStream. Ensuite, nous voulons compter ces mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pairs = words.map(word => (word, 1))\n",
    "val wordCounts = pairs.reduceByKey(_ + _)\n",
    "\n",
    "// Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les mots DStream sont en outre mappés (transformation un à un) en un flux DStream de (mot, 1) paires, qui est ensuite réduit pour obtenir la fréquence des mots dans chaque lot de données. Enfin, wordCounts.print () imprimera quelques-uns des comptes générés chaque seconde.\n",
    "\n",
    "Notez que lorsque ces lignes sont exécutées, Spark Streaming configure uniquement le calcul qu'il effectuera au démarrage et aucun traitement réel n'a encore commencé. Pour commencer le traitement après que toutes les transformations ont été configurées, nous appelons finalement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()             // Start the computation\n",
    "ssc.awaitTermination()  // Wait for the computation to terminate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
